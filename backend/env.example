# OpenAI API Key (Required)
OPENAI_API_KEY=your_openai_api_key_here

# Google Maps API Key (Optional - for distance calculations)
GOOGLE_MAPS_API_KEY=your_google_maps_api_key_here

# Database
DB_PATH=../db/travel_data.db

# JWT Secret (Change in production)
JWT_SECRET=your-secret-key-change-in-production

# CO2 Service URL
CO2_SERVICE_URL=http://localhost:5001

# Agent Configuration
AGENT_MODEL=gpt-4o
AGENT_TEMPERATURE=0.1
AGENT_MAX_TOKENS=2000

# Fine-tuned Model Override (optional)
# Set to base model used for LoRA adapters, e.g. mistralai/Mistral-7B-Instruct-v0.1
# If unset, service will attempt to read adapter_config.json
FINETUNED_BASE_MODEL=

# Fine-tuned Model Path (optional)
# Set to a local path for adapter or base model directory
# e.g. ../finetuning/models/travel-sustainability-lora or ../finetuning/models/mistral-7b-instruct
FINETUNED_MODEL_PATH=

# Fine-tuned Model Variant Selection
# Allowed: base_4bit, colab_finetune
FINETUNED_MODEL_VARIANT=base_4bit

# Optional explicit override (takes precedence over variant)
FINETUNED_MODEL_PATH=

# Absolute paths for the two supported variants
FINETUNED_MODEL_PATH_BASE_4BIT=/Users/arpita/Documents/project/finetuning/models/mistral-7b-instruct-4bit

# When variant is colab_finetune, if the extract dir doesn't exist, we will
# extract the ZIP once into this directory and use it as the adapter path
FINETUNED_MODEL_ZIP_PATH_COLAB=/Users/arpita/Documents/project/data/model/mistral_lora.zip
FINETUNED_MODEL_PATH_COLAB_EXTRACT=/Users/arpita/Documents/project/finetuning/models/mistral-7b-finetune-4bit-colab

# MLX (macOS) Inference
# Enable to use MLX-backed Mistral on macOS for this service
USE_MLX=false

# Simple model switch for MLX inference
# community  -> loads the base community model (MLX_MODEL)
# finetuned  -> loads base model (MLX_BASE_MODEL) with LoRA adapters (MLX_ADAPTER_PATH)
MODEL_MODE=community

# Base community model (Hugging Face repo id or local MLX-converted dir)
MLX_MODEL=mlx-community/Mistral-7B-Instruct-v0.2-4bit

# When MODEL_MODE=finetuned, set both of the following:
# - MLX_BASE_MODEL: base model to load before applying adapters
# - MLX_ADAPTER_PATH: local directory containing adapters.safetensors and adapter_config.json
MLX_BASE_MODEL=mlx-community/Mistral-7B-Instruct-v0.2-4bit
MLX_ADAPTER_PATH=/Users/arpita/Documents/project/finetuning/models/mistral-mlx-lora
